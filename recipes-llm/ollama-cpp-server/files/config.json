{
  "server": {
    "host": "0.0.0.0",
    "port": 11434,
    "timeout": 300
  },
  "models": {
    "path": "/var/lib/ollama/models",
    "default_context_size": 2048,
    "default_threads": 4
  },
  "inference": {
    "gpu_layers": 0,
    "batch_size": 512,
    "temperature": 0.8,
    "top_p": 0.9,
    "top_k": 40,
    "repeat_penalty": 1.1
  },
  "logging": {
    "level": "info",
    "format": "text"
  }
}
