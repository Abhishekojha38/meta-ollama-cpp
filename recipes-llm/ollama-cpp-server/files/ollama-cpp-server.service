[Unit]
Description=Ollama-compatible LLM Server (llama.cpp)
After=network.target
Documentation=https://github.com/ggerganov/llama.cpp

[Service]
Type=simple
User=ollama
Group=ollama
Environment="OLLAMA_HOST=0.0.0.0:11434"
Environment="OLLAMA_MODELS=/var/lib/ollama/models"
ExecStart=/usr/bin/ollama-cpp-server
Restart=on-failure
RestartSec=10
StandardOutput=journal
StandardError=journal

# Security settings
NoNewPrivileges=true
PrivateTmp=true
ProtectSystem=strict
ProtectHome=true
ReadWritePaths=/var/lib/ollama

# Resource limits (adjust based on your device)
MemoryLimit=4G
CPUQuota=200%

[Install]
WantedBy=multi-user.target
